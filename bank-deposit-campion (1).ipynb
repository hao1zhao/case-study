{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of contents\n***\n\n* [1. Introduction](#1)\n* [2. Data set review & preparation](#2)\n* [3. Exploratory Data Analysis](#3)\n* [4. Model fitting and selection](#4)\n* [5. Conclusion](#5)\n","metadata":{}},{"cell_type":"markdown","source":"# <font color=\"#00bfff\"> 1. Introduction </font>\n<a id=\"1\"></a> \n***","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom plotly import tools\nimport matplotlib.pyplot as plt\n# import plotly.plotly as py\n# import plotly.figure_factory as ff\n# import plotly.graph_objs as go\n# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n# init_notebook_mode(connected=True)\ndf = pd.read_csv('../input/bank-data-set/bank.csv')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:18.539605Z","iopub.execute_input":"2022-07-05T17:43:18.540325Z","iopub.status.idle":"2022-07-05T17:43:20.183533Z","shell.execute_reply.started":"2022-07-05T17:43:18.540231Z","shell.execute_reply":"2022-07-05T17:43:20.182535Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.185813Z","iopub.execute_input":"2022-07-05T17:43:20.186504Z","iopub.status.idle":"2022-07-05T17:43:20.231122Z","shell.execute_reply.started":"2022-07-05T17:43:20.186465Z","shell.execute_reply":"2022-07-05T17:43:20.229700Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Attributes Description:\n\n1 - **age:** (numeric)<br>\n2 - **job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n3 - **marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n4 - **education:** (categorical: primary, secondary, tertiary and unknown)<br>\n5 - **default:** has credit in default? (categorical: 'no','yes','unknown')<br>\n6 - **housing:** has housing loan? (categorical: 'no','yes','unknown')<br>\n7 - **loan:** has personal loan? (categorical: 'no','yes','unknown')<br>\n8 - **balance:** Balance of the individual.\n\n8 - **contact:** contact communication type (categorical: 'cellular','telephone') <br>\n9 - **month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n10 - **day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n11 - **duration:** last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>\n\n12 - **campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n13 - **pdays:** number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>\n14 - **previous:** number of contacts performed before this campaign and for this client (numeric)<br>\n15 - **poutcome:** outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n\nOutput variable (desired target):<br>\n21 - **y** - has the client subscribed a term deposit? (binary: 'yes','no')","metadata":{}},{"cell_type":"markdown","source":"# <font color=\"#00bfff\"> 2. Data set review & preparation\n<a id=\"2\"></a> \n***","metadata":{}},{"cell_type":"code","source":"#Date cleaning\n#Missing value \nmissing_values_count = df.isnull().sum()\nmissing_values_count","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.232814Z","iopub.execute_input":"2022-07-05T17:43:20.233156Z","iopub.status.idle":"2022-07-05T17:43:20.257663Z","shell.execute_reply.started":"2022-07-05T17:43:20.233124Z","shell.execute_reply":"2022-07-05T17:43:20.256448Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"By looking at the documentation, I can see that pdays has missed some information, but all other information are intact. This means that these values are probably missing because they were not recorded, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.","metadata":{}},{"cell_type":"code","source":"pdays_median = df['pdays'].median()\npdays_mean = df['pdays'].mean()\nprint('mean',round(pdays_mean,2))\nprint('median',pdays_median)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.263848Z","iopub.execute_input":"2022-07-05T17:43:20.267629Z","iopub.status.idle":"2022-07-05T17:43:20.279931Z","shell.execute_reply.started":"2022-07-05T17:43:20.267561Z","shell.execute_reply":"2022-07-05T17:43:20.278551Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# How many total missing values do we have?\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n# Percent of data that is missing\npercent_missing = (total_missing/total_cells) * 100\nprint(round(percent_missing * 100,2), '%')","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.282060Z","iopub.execute_input":"2022-07-05T17:43:20.285548Z","iopub.status.idle":"2022-07-05T17:43:20.295839Z","shell.execute_reply.started":"2022-07-05T17:43:20.285499Z","shell.execute_reply":"2022-07-05T17:43:20.294826Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# The missing value accounts for 0.37%, I would like to use median to fill the missing part.\ndf = df.fillna(pdays_median)\nterm_deposits = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.297180Z","iopub.execute_input":"2022-07-05T17:43:20.298074Z","iopub.status.idle":"2022-07-05T17:43:20.324538Z","shell.execute_reply.started":"2022-07-05T17:43:20.298024Z","shell.execute_reply":"2022-07-05T17:43:20.323138Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# No more missing value\nmissing_values_count = df.isnull().sum()\nmissing_values_count ","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.328906Z","iopub.execute_input":"2022-07-05T17:43:20.329329Z","iopub.status.idle":"2022-07-05T17:43:20.357222Z","shell.execute_reply.started":"2022-07-05T17:43:20.329291Z","shell.execute_reply":"2022-07-05T17:43:20.355651Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#00bfff\"> 3. Exploratory Data Analysis\n<a id=\"3\"></a> \n***","metadata":{}},{"cell_type":"code","source":"# Get list of categorical variables\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.360848Z","iopub.execute_input":"2022-07-05T17:43:20.361301Z","iopub.status.idle":"2022-07-05T17:43:20.372952Z","shell.execute_reply.started":"2022-07-05T17:43:20.361262Z","shell.execute_reply":"2022-07-05T17:43:20.371848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.374651Z","iopub.execute_input":"2022-07-05T17:43:20.375033Z","iopub.status.idle":"2022-07-05T17:43:20.405161Z","shell.execute_reply.started":"2022-07-05T17:43:20.375003Z","shell.execute_reply":"2022-07-05T17:43:20.403982Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Let's see how the numeric data is distributed.\nplt.style.use('seaborn-paper')\ndf.hist(bins=15, figsize=(14,10), color='#0072BD')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:20.410104Z","iopub.execute_input":"2022-07-05T17:43:20.411476Z","iopub.status.idle":"2022-07-05T17:43:21.839429Z","shell.execute_reply.started":"2022-07-05T17:43:20.411437Z","shell.execute_reply":"2022-07-05T17:43:21.838254Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.deposit = df.deposit.map({'no':0, 'yes':1}).astype('uint8')","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:21.841426Z","iopub.execute_input":"2022-07-05T17:43:21.841900Z","iopub.status.idle":"2022-07-05T17:43:21.852825Z","shell.execute_reply.started":"2022-07-05T17:43:21.841860Z","shell.execute_reply":"2022-07-05T17:43:21.851667Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#correlation matrix\nplt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), cmap=\"YlGnBu\",annot=True, fmt='.2f', vmin=0);","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:21.854130Z","iopub.execute_input":"2022-07-05T17:43:21.855352Z","iopub.status.idle":"2022-07-05T17:43:22.315880Z","shell.execute_reply.started":"2022-07-05T17:43:21.855312Z","shell.execute_reply":"2022-07-05T17:43:22.314614Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* most correlated with target feature is call duration. So we need to transform it to reduce the influence\n* higly correlated features (pdays and previous) may describe the relationship between campaign and client contact from different angles. Their variance might support model capacity for generalization.","metadata":{}},{"cell_type":"markdown","source":"<h3> Analysis by Job: </h3>\n","metadata":{}},{"cell_type":"code","source":"# Build a function to show categorical values disribution\ndef plot_bar(column):\n    # temp df \n    temp_1 = pd.DataFrame()\n    # count categorical values\n    temp_1['No_deposit'] = term_deposits[term_deposits['deposit'] == 'no'][column].value_counts()\n    temp_1['Yes_deposit'] = term_deposits[term_deposits['deposit'] == 'yes'][column].value_counts()\n    temp_1.plot(kind='bar',stacked=True)\n    plt.xlabel(f'{column}')\n    plt.ylabel('Number of clients')\n    plt.title('Distribution of {} and deposit'.format(column))\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:22.317195Z","iopub.execute_input":"2022-07-05T17:43:22.317527Z","iopub.status.idle":"2022-07-05T17:43:22.325953Z","shell.execute_reply.started":"2022-07-05T17:43:22.317495Z","shell.execute_reply":"2022-07-05T17:43:22.324899Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plot_bar('job'), plot_bar('marital'), plot_bar('education'), plot_bar('contact'), plot_bar('loan'), plot_bar('housing')","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:22.328978Z","iopub.execute_input":"2022-07-05T17:43:22.329356Z","iopub.status.idle":"2022-07-05T17:43:23.597677Z","shell.execute_reply.started":"2022-07-05T17:43:22.329332Z","shell.execute_reply":"2022-07-05T17:43:23.596670Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"* Administrative staff and technical specialists opened the deposit most of all. In relative terms, a high proportion of pensioners and students might be mentioned as well.\n* Best communication channel is secullar.\n* The difference is evident between consumers who already use the services of banks and received a loan.\n* Home ownership does affect marketing company performance.","metadata":{}},{"cell_type":"markdown","source":"<h3> Campaign Duration: </h3>","metadata":{}},{"cell_type":"markdown","source":"**outlier**","metadata":{}},{"cell_type":"code","source":"sns.displot(df['duration'],kind=\"ecdf\")","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:23.598976Z","iopub.execute_input":"2022-07-05T17:43:23.599283Z","iopub.status.idle":"2022-07-05T17:43:23.837586Z","shell.execute_reply.started":"2022-07-05T17:43:23.599255Z","shell.execute_reply":"2022-07-05T17:43:23.836142Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The average campaign duration is 374.76, I think the outlier above 3500 should be excluded.  Long dial's holding time and frequently change receptionist maybe the reason that outlier duration value is too large. ","metadata":{}},{"cell_type":"code","source":"#Remove everything above 99%\nterm_duration = term_deposits.copy()\nterm_duration= term_duration.loc[term_duration['duration'] <= np.quantile(term_duration['duration'],q=0.99)]","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:23.839394Z","iopub.execute_input":"2022-07-05T17:43:23.839726Z","iopub.status.idle":"2022-07-05T17:43:23.850435Z","shell.execute_reply.started":"2022-07-05T17:43:23.839694Z","shell.execute_reply":"2022-07-05T17:43:23.848963Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.box(term_duration, x=\"job\", y=\"duration\",color=\"deposit\")\nfig.update_traces(quartilemethod=\"exclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:23.851921Z","iopub.execute_input":"2022-07-05T17:43:23.852857Z","iopub.status.idle":"2022-07-05T17:43:26.851251Z","shell.execute_reply.started":"2022-07-05T17:43:23.852820Z","shell.execute_reply":"2022-07-05T17:43:26.849991Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"* The leads who have not made a deposit have lesser duration on calls\n* Comparing the average, the blue collar, entrepreneur have high duration in calls and student, retired have less duration in average\n* Large distribution of leads were from self employed clients and management people.","metadata":{}},{"cell_type":"markdown","source":"# <font color=\"#00bfff\"> 4. Model fitting and selection\n<a id=\"4\"></a> \n***","metadata":{}},{"cell_type":"markdown","source":"## Stratified Sampling: ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n# Here we split the data into training and test sets and implement a stratified shuffle split.\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_set, test_set in stratified.split(df, df[\"loan\"]):\n    stratified_train = df.loc[train_set]\n    stratified_test = df.loc[test_set]\n    \nstratified_train[\"loan\"].value_counts()/len(df)\nstratified_test[\"loan\"].value_counts()/len(df)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:26.852525Z","iopub.execute_input":"2022-07-05T17:43:26.852813Z","iopub.status.idle":"2022-07-05T17:43:26.956196Z","shell.execute_reply.started":"2022-07-05T17:43:26.852785Z","shell.execute_reply":"2022-07-05T17:43:26.954952Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Separate the labels and the features.\ntrain_data = stratified_train # Make a copy of the stratified training set.\ntest_data = stratified_test\ntrain_data.shape\ntest_data.shape\ntrain_data['deposit'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:26.957646Z","iopub.execute_input":"2022-07-05T17:43:26.958041Z","iopub.status.idle":"2022-07-05T17:43:26.968489Z","shell.execute_reply.started":"2022-07-05T17:43:26.958012Z","shell.execute_reply":"2022-07-05T17:43:26.967622Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n       \n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:26.970231Z","iopub.execute_input":"2022-07-05T17:43:26.970660Z","iopub.status.idle":"2022-07-05T17:43:27.000637Z","shell.execute_reply.started":"2022-07-05T17:43:26.970636Z","shell.execute_reply":"2022-07-05T17:43:26.999455Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.002079Z","iopub.execute_input":"2022-07-05T17:43:27.002437Z","iopub.status.idle":"2022-07-05T17:43:27.020764Z","shell.execute_reply.started":"2022-07-05T17:43:27.002406Z","shell.execute_reply":"2022-07-05T17:43:27.019604Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Making pipelines\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector([\"age\", \"balance\", \"day\", \"campaign\", \"pdays\", \"previous\",\"duration\"])),\n    (\"std_scaler\", StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector([\"job\", \"education\", \"marital\", \"default\", \"housing\", \"loan\", \"contact\", \"month\",\n                                     \"poutcome\"])),\n    (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense'))\n])\n\nfrom sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.022741Z","iopub.execute_input":"2022-07-05T17:43:27.023398Z","iopub.status.idle":"2022-07-05T17:43:27.044217Z","shell.execute_reply.started":"2022-07-05T17:43:27.023365Z","shell.execute_reply":"2022-07-05T17:43:27.043196Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"X_train = preprocess_pipeline.fit_transform(train_data)\nX_train","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.045813Z","iopub.execute_input":"2022-07-05T17:43:27.046147Z","iopub.status.idle":"2022-07-05T17:43:27.139078Z","shell.execute_reply.started":"2022-07-05T17:43:27.046119Z","shell.execute_reply":"2022-07-05T17:43:27.138101Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"y_train = train_data['deposit']\ny_test = test_data['deposit']\ny_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.140650Z","iopub.execute_input":"2022-07-05T17:43:27.141062Z","iopub.status.idle":"2022-07-05T17:43:27.147923Z","shell.execute_reply.started":"2022-07-05T17:43:27.141015Z","shell.execute_reply":"2022-07-05T17:43:27.146938Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\ny_train = encode.fit_transform(y_train)\ny_test = encode.fit_transform(y_test)\ny_train_yes = (y_train == 1)\ny_train\ny_train_yes","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.149450Z","iopub.execute_input":"2022-07-05T17:43:27.150765Z","iopub.status.idle":"2022-07-05T17:43:27.163261Z","shell.execute_reply.started":"2022-07-05T17:43:27.150727Z","shell.execute_reply":"2022-07-05T17:43:27.162128Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"some_instance = X_train[1250]","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.164693Z","iopub.execute_input":"2022-07-05T17:43:27.165928Z","iopub.status.idle":"2022-07-05T17:43:27.173217Z","shell.execute_reply.started":"2022-07-05T17:43:27.165881Z","shell.execute_reply":"2022-07-05T17:43:27.172087Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Time for Classification Models\nimport time\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Decision Tree\": tree.DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=18),\n    \"Neural Net\": MLPClassifier(alpha=1),\n    \"Naive Bayes\": GaussianNB()\n}","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.175630Z","iopub.execute_input":"2022-07-05T17:43:27.176562Z","iopub.status.idle":"2022-07-05T17:43:27.451381Z","shell.execute_reply.started":"2022-07-05T17:43:27.176524Z","shell.execute_reply":"2022-07-05T17:43:27.450192Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#  Thanks to Ahspinar for the function. \nno_classifiers = len(dict_classifiers.keys())\n\ndef batch_classify(X_train, Y_train, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['classifier', 'train_score', 'training_time'])\n    count = 0\n    for key, classifier in dict_classifiers.items():\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        df_results.loc[count,'classifier'] = key\n        df_results.loc[count,'train_score'] = train_score\n        df_results.loc[count,'training_time'] = t_diff\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_diff))\n        count+=1\n    return df_results","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.460102Z","iopub.execute_input":"2022-07-05T17:43:27.460452Z","iopub.status.idle":"2022-07-05T17:43:27.471571Z","shell.execute_reply.started":"2022-07-05T17:43:27.460424Z","shell.execute_reply":"2022-07-05T17:43:27.470085Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_results = batch_classify(X_train, y_train)\nprint(df_results.sort_values(by='train_score', ascending=False))","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:27.474367Z","iopub.execute_input":"2022-07-05T17:43:27.474962Z","iopub.status.idle":"2022-07-05T17:43:48.430995Z","shell.execute_reply.started":"2022-07-05T17:43:27.474938Z","shell.execute_reply":"2022-07-05T17:43:48.430059Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Use Cross-validation.\nfrom sklearn.model_selection import cross_val_score\n\n# Logistic Regression\nlog_reg = LogisticRegression()\nlog_scores = cross_val_score(log_reg, X_train, y_train, cv=3)\nlog_reg_mean = log_scores.mean()\n\n# SVC\nsvc_clf = SVC()\nsvc_scores = cross_val_score(svc_clf, X_train, y_train, cv=3)\nsvc_mean = svc_scores.mean()\n\n# KNearestNeighbors\nknn_clf = KNeighborsClassifier()\nknn_scores = cross_val_score(knn_clf, X_train, y_train, cv=3)\nknn_mean = knn_scores.mean()\n\n# Decision Tree\ntree_clf = tree.DecisionTreeClassifier()\ntree_scores = cross_val_score(tree_clf, X_train, y_train, cv=3)\ntree_mean = tree_scores.mean()\n\n# Gradient Boosting Classifier\ngrad_clf = GradientBoostingClassifier()\ngrad_scores = cross_val_score(grad_clf, X_train, y_train, cv=3)\ngrad_mean = grad_scores.mean()\n\n# Random Forest Classifier\nrand_clf = RandomForestClassifier(n_estimators=18)\nrand_scores = cross_val_score(rand_clf, X_train, y_train, cv=3)\nrand_mean = rand_scores.mean()\n\n# NeuralNet Classifier\nneural_clf = MLPClassifier(alpha=1)\nneural_scores = cross_val_score(neural_clf, X_train, y_train, cv=3)\nneural_mean = neural_scores.mean()\n\n# Naives Bayes\nnav_clf = GaussianNB()\nnav_scores = cross_val_score(nav_clf, X_train, y_train, cv=3)\nnav_mean = neural_scores.mean()\n\n# Create a Dataframe with the results.\nd = {'Classifiers': ['Logistic Reg.', 'SVC', 'KNN', 'Dec Tree', 'Grad B CLF', 'Rand FC', 'Neural Classifier', 'Naives Bayes'], \n    'Crossval Mean Scores': [log_reg_mean, svc_mean, knn_mean, tree_mean, grad_mean, rand_mean, neural_mean, nav_mean]}\n\nresult_df = pd.DataFrame(data=d)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:43:48.433856Z","iopub.execute_input":"2022-07-05T17:43:48.434176Z","iopub.status.idle":"2022-07-05T17:44:12.567724Z","shell.execute_reply.started":"2022-07-05T17:43:48.434147Z","shell.execute_reply":"2022-07-05T17:44:12.566817Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# All our models perform well but I will go with GradientBoosting.\nresult_df = result_df.sort_values(by=['Crossval Mean Scores'], ascending=False)\nresult_df","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:12.568954Z","iopub.execute_input":"2022-07-05T17:44:12.569702Z","iopub.status.idle":"2022-07-05T17:44:12.583567Z","shell.execute_reply.started":"2022-07-05T17:44:12.569668Z","shell.execute_reply":"2022-07-05T17:44:12.582491Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Cross validate our Gradient Boosting Classifier\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(grad_clf, X_train, y_train, cv=3)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:12.585330Z","iopub.execute_input":"2022-07-05T17:44:12.585893Z","iopub.status.idle":"2022-07-05T17:44:16.616498Z","shell.execute_reply.started":"2022-07-05T17:44:12.585859Z","shell.execute_reply":"2022-07-05T17:44:16.614620Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ngrad_clf.fit(X_train, y_train)\nprint (\"Gradient Boost Classifier accuracy is %2.2f\" % accuracy_score(y_train, y_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:16.618508Z","iopub.execute_input":"2022-07-05T17:44:16.618945Z","iopub.status.idle":"2022-07-05T17:44:18.632815Z","shell.execute_reply.started":"2022-07-05T17:44:16.618907Z","shell.execute_reply":"2022-07-05T17:44:18.631822Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Let's find the scores  for precision and recall.\nfrom sklearn.metrics import precision_score, recall_score\n# The model is 77% sure that the potential client will suscribe to a term deposit. \n# The model is only retaining 60% of clients that agree to suscribe a term deposit.\nprint('Precision Score: ', precision_score(y_train, y_train_pred))\n# The classifier only detects 60% of potential clients that will suscribe to a term deposit.\nprint('Recall Score: ', recall_score(y_train, y_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:18.634021Z","iopub.execute_input":"2022-07-05T17:44:18.634264Z","iopub.status.idle":"2022-07-05T17:44:18.647692Z","shell.execute_reply.started":"2022-07-05T17:44:18.634241Z","shell.execute_reply":"2022-07-05T17:44:18.646212Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(y_train, y_train_pred)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:18.649198Z","iopub.execute_input":"2022-07-05T17:44:18.649463Z","iopub.status.idle":"2022-07-05T17:44:18.659502Z","shell.execute_reply.started":"2022-07-05T17:44:18.649442Z","shell.execute_reply":"2022-07-05T17:44:18.658636Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"y_scores = grad_clf.decision_function([some_instance])\ny_scores","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:18.661197Z","iopub.execute_input":"2022-07-05T17:44:18.661673Z","iopub.status.idle":"2022-07-05T17:44:18.672394Z","shell.execute_reply.started":"2022-07-05T17:44:18.661579Z","shell.execute_reply":"2022-07-05T17:44:18.670559Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Increasing the threshold decreases the recall.\nthreshold = 0\ny_some_digit_pred = (y_scores > threshold)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:18.673796Z","iopub.execute_input":"2022-07-05T17:44:18.674733Z","iopub.status.idle":"2022-07-05T17:44:18.680919Z","shell.execute_reply.started":"2022-07-05T17:44:18.674673Z","shell.execute_reply":"2022-07-05T17:44:18.679838Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"y_scores = cross_val_predict(grad_clf, X_train, y_train, cv=3, method=\"decision_function\")\nneural_y_scores = cross_val_predict(neural_clf, X_train, y_train, cv=3, method=\"predict_proba\")\nnaives_y_scores = cross_val_predict(nav_clf, X_train, y_train, cv=3, method=\"predict_proba\")","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:18.682352Z","iopub.execute_input":"2022-07-05T17:44:18.683207Z","iopub.status.idle":"2022-07-05T17:44:33.165280Z","shell.execute_reply.started":"2022-07-05T17:44:18.683177Z","shell.execute_reply":"2022-07-05T17:44:33.164374Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# hack to work around issue #9589 introduced in Scikit-Learn 0.19.0\nif y_scores.ndim == 2:\n    y_scores = y_scores[:, 1]\n\nif neural_y_scores.ndim == 2:\n    neural_y_scores = neural_y_scores[:, 1]\n    \nif naives_y_scores.ndim == 2:\n    naives_y_scores = naives_y_scores[:, 1]","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.169632Z","iopub.execute_input":"2022-07-05T17:44:33.172469Z","iopub.status.idle":"2022-07-05T17:44:33.178604Z","shell.execute_reply.started":"2022-07-05T17:44:33.172412Z","shell.execute_reply":"2022-07-05T17:44:33.177669Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"y_scores.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.180258Z","iopub.execute_input":"2022-07-05T17:44:33.180913Z","iopub.status.idle":"2022-07-05T17:44:33.193641Z","shell.execute_reply.started":"2022-07-05T17:44:33.180875Z","shell.execute_reply":"2022-07-05T17:44:33.192945Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# How can we decide which threshold to use? We want to return the scores instead of predictions with this code.\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, threshold = precision_recall_curve(y_train, y_scores)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.194659Z","iopub.execute_input":"2022-07-05T17:44:33.197205Z","iopub.status.idle":"2022-07-05T17:44:33.207143Z","shell.execute_reply.started":"2022-07-05T17:44:33.197174Z","shell.execute_reply":"2022-07-05T17:44:33.206335Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def precision_recall_curve(precisions, recalls, thresholds):\n    fig, ax = plt.subplots(figsize=(12,8))\n    plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precisions\")\n    plt.plot(thresholds, recalls[:-1], \"#424242\", label=\"Recalls\")\n    plt.title(\"Precision and Recall \\n Tradeoff\", fontsize=18)\n    plt.ylabel(\"Level of Precision and Recall\", fontsize=16)\n    plt.xlabel(\"Thresholds\", fontsize=16)\n    plt.legend(loc=\"best\", fontsize=14)\n    plt.xlim([-2, 4.7])\n    plt.ylim([0, 1])\n    plt.axvline(x=0.13, linewidth=3, color=\"#0B3861\")\n    plt.annotate('Best Precision and \\n Recall Balance \\n is at 0.13 \\n threshold ', xy=(0.13, 0.83), xytext=(55, -40),\n             textcoords=\"offset points\",\n            arrowprops=dict(facecolor='black', shrink=0.05),\n                fontsize=12, \n                color='k')\n    \nprecision_recall_curve(precisions, recalls, threshold)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.208371Z","iopub.execute_input":"2022-07-05T17:44:33.210174Z","iopub.status.idle":"2022-07-05T17:44:33.460571Z","shell.execute_reply.started":"2022-07-05T17:44:33.210139Z","shell.execute_reply":"2022-07-05T17:44:33.459192Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# ROC Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n# Gradient Boosting Classifier\n# Neural Classifier\n# Naives Bayes Classifier\ngrd_fpr, grd_tpr, thresold = roc_curve(y_train, y_scores)\nneu_fpr, neu_tpr, neu_threshold = roc_curve(y_train, neural_y_scores)\nnav_fpr, nav_tpr, nav_threshold = roc_curve(y_train, naives_y_scores)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.462058Z","iopub.execute_input":"2022-07-05T17:44:33.462381Z","iopub.status.idle":"2022-07-05T17:44:33.475251Z","shell.execute_reply.started":"2022-07-05T17:44:33.462353Z","shell.execute_reply":"2022-07-05T17:44:33.473747Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def graph_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.figure(figsize=(10,6))\n    plt.title('ROC Curve \\n Gradient Boosting Classifier', fontsize=18)\n    plt.plot(false_positive_rate, true_positive_rate, label=label)\n    plt.plot([0, 1], [0, 1], '#0C8EE0')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('ROC Score of 91.73% \\n (Not the best score)', xy=(0.25, 0.9), xytext=(0.4, 0.85),\n            arrowprops=dict(facecolor='#F75118', shrink=0.05),\n            )\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#F75118', shrink=0.05),\n                )\n    \n    \ngraph_roc_curve(grd_fpr, grd_tpr, threshold)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.476957Z","iopub.execute_input":"2022-07-05T17:44:33.477671Z","iopub.status.idle":"2022-07-05T17:44:33.701374Z","shell.execute_reply.started":"2022-07-05T17:44:33.477638Z","shell.execute_reply":"2022-07-05T17:44:33.699374Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Gradient Boost Classifier Score: ', roc_auc_score(y_train, y_scores))\nprint('Neural Classifier Score: ', roc_auc_score(y_train, neural_y_scores))\nprint('Naives Bayes Classifier: ', roc_auc_score(y_train, naives_y_scores))","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.703447Z","iopub.execute_input":"2022-07-05T17:44:33.704699Z","iopub.status.idle":"2022-07-05T17:44:33.725434Z","shell.execute_reply.started":"2022-07-05T17:44:33.704636Z","shell.execute_reply":"2022-07-05T17:44:33.723694Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def graph_roc_curve_multiple(grd_fpr, grd_tpr, neu_fpr, neu_tpr, nav_fpr, nav_tpr):\n    plt.figure(figsize=(8,6))\n    plt.title('ROC Curve \\n Top 3 Classifiers', fontsize=18)\n    plt.plot(grd_fpr, grd_tpr, label='Gradient Boosting Classifier (Score = 91.72%)')\n    plt.plot(neu_fpr, neu_tpr, label='Neural Classifier (Score = 91.54%)')\n    plt.plot(nav_fpr, nav_tpr, label='Naives Bayes Classifier (Score = 80.33%)')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(grd_fpr, grd_tpr, neu_fpr, neu_tpr, nav_fpr, nav_tpr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:33.726986Z","iopub.execute_input":"2022-07-05T17:44:33.728048Z","iopub.status.idle":"2022-07-05T17:44:34.007454Z","shell.execute_reply.started":"2022-07-05T17:44:33.728015Z","shell.execute_reply":"2022-07-05T17:44:34.006239Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"grad_clf.predict_proba([some_instance])","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:34.009622Z","iopub.execute_input":"2022-07-05T17:44:34.010017Z","iopub.status.idle":"2022-07-05T17:44:34.019881Z","shell.execute_reply.started":"2022-07-05T17:44:34.009989Z","shell.execute_reply":"2022-07-05T17:44:34.017862Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Let's see what does our classifier predict.\ngrad_clf.predict([some_instance]) ","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:34.021499Z","iopub.execute_input":"2022-07-05T17:44:34.022274Z","iopub.status.idle":"2022-07-05T17:44:34.038575Z","shell.execute_reply.started":"2022-07-05T17:44:34.022242Z","shell.execute_reply":"2022-07-05T17:44:34.037117Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"y_train[1250]","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:34.040961Z","iopub.execute_input":"2022-07-05T17:44:34.041363Z","iopub.status.idle":"2022-07-05T17:44:34.051241Z","shell.execute_reply.started":"2022-07-05T17:44:34.041325Z","shell.execute_reply":"2022-07-05T17:44:34.050253Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Which Features Influence the Result of a Term Deposit Suscription?\n## DecisionTreeClassifier:\n<a id=\"decision\"></a>\nThe top three most important features for our classifier are **Duration (how long it took the conversation between the sales representative and the potential client), contact (number of contacts to the potential client within the same marketing campaign), month (the month of the year).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nplt.style.use('seaborn-white')\n\n# Convert the columns into categorical variables\nterm_deposits['job'] = term_deposits['job'].astype('category').cat.codes\nterm_deposits['marital'] = term_deposits['marital'].astype('category').cat.codes\nterm_deposits['education'] = term_deposits['education'].astype('category').cat.codes\nterm_deposits['contact'] = term_deposits['contact'].astype('category').cat.codes\nterm_deposits['poutcome'] = term_deposits['poutcome'].astype('category').cat.codes\nterm_deposits['month'] = term_deposits['month'].astype('category').cat.codes\nterm_deposits['default'] = term_deposits['default'].astype('category').cat.codes\nterm_deposits['loan'] = term_deposits['loan'].astype('category').cat.codes\nterm_deposits['housing'] = term_deposits['housing'].astype('category').cat.codes\n\n# Let's create new splittings like before but now we modified the data so we need to do it one more time.\n# Create train and test splits\ntarget_name = 'deposit'\nX = term_deposits.drop('deposit', axis=1)\n\n\nlabel=term_deposits[target_name]\n\nX_train, X_test, y_train, y_test = train_test_split(X,label,test_size=0.2, random_state=42, stratify=label)\n\n# Build a classification task using 3 informative features\ntree = tree.DecisionTreeClassifier(\n    class_weight='balanced',\n    min_weight_fraction_leaf = 0.01\n    \n)\n\n\n\ntree = tree.fit(X_train, y_train)\nimportances = tree.feature_importances_\nfeature_names = term_deposits.drop('deposit', axis=1).columns\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\ndef feature_importance_graph(indices, importances, feature_names):\n    plt.figure(figsize=(12,6))\n    plt.title(\"Determining Feature importances \\n with DecisionTreeClassifier\", fontsize=18)\n    plt.barh(range(len(indices)), importances[indices], color='#31B173',  align=\"center\")\n    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)\n    plt.ylim([-1, len(indices)])\n    plt.axhline(y=1.85, xmin=0.21, xmax=0.952, color='k', linewidth=3, linestyle='--')\n    plt.text(0.30, 2.8, '46% Difference between \\n duration and contacts', color='k', fontsize=15)\n    \nfeature_importance_graph(indices, importances, feature_names)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:34.052725Z","iopub.execute_input":"2022-07-05T17:44:34.053086Z","iopub.status.idle":"2022-07-05T17:44:34.396193Z","shell.execute_reply.started":"2022-07-05T17:44:34.053050Z","shell.execute_reply":"2022-07-05T17:44:34.395533Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"A voting classifier is a machine learning estimator that trains various base models or estimators and predicts on the basis of aggregating the findings of each base estimator. The aggregating criteria can be combined decision of voting for each estimator output. The voting criteria can be of two types:\n\n* Hard Voting: Voting is calculated on the predicted output class.\n* Soft Voting: Voting is calculated on the predicted probability of the output class.","metadata":{}},{"cell_type":"code","source":"# Our three classifiers are grad_clf, nav_clf and neural_clf\nfrom sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(\n    estimators=[('gbc', grad_clf), ('nav', nav_clf), ('neural', neural_clf)],\n    voting='soft'\n)\n\nvoting_clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:34.397089Z","iopub.execute_input":"2022-07-05T17:44:34.397495Z","iopub.status.idle":"2022-07-05T17:44:36.962523Z","shell.execute_reply.started":"2022-07-05T17:44:34.397473Z","shell.execute_reply":"2022-07-05T17:44:36.961808Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor clf in (grad_clf, nav_clf, neural_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    predict = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, predict))","metadata":{"execution":{"iopub.status.busy":"2022-07-05T17:44:36.963468Z","iopub.execute_input":"2022-07-05T17:44:36.963894Z","iopub.status.idle":"2022-07-05T17:44:43.853309Z","shell.execute_reply.started":"2022-07-05T17:44:36.963866Z","shell.execute_reply":"2022-07-05T17:44:43.852465Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# <font color=\"#00bfff\"> 5. Conclusion\n<a id=\"5\"></a> \n***","metadata":{}},{"cell_type":"markdown","source":"**Gradient Boosting** classifier is the best model (84% accuracy) to predict whether or not a potential client will suscribe to a term deposit or not. \n\n\n* Focus on specific categories. The model shows that students and senior citizens respond better to proposal.So I would suggest focusing on these two groups and create different financial programs and marketing messages in advertising for each.\n* I would recommend creating a marketing campaign for people who didn't have any credits before (explaining how it works and what are the benefits).Recommendations for the Sales Department (Call Center)\n* Always contact clients by cellphone when possible\n* Target individuals with a higher duration,longer phone conversations perform better, so try to keep a conversation going as much as you can\n","metadata":{}}]}